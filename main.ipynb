{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\RUSHITA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import TextFormatter\n",
    "import google.generativeai as genai\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Configure API keys\n",
    "GEMINI_API_KEY = os.getenv('GeminiAPI', 'AIzaSyDy1RrOdnAeX3t0XTLeH_Qlq8XsAloK98M')\n",
    "genai.configure(api_key=GEMINI_API_KEY)\n",
    "\n",
    "# Load NLP model for semantic chunking\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class YouTubeQASystem:\n",
    "    def __init__(self, db_path=\"my_vectordb\", collection_name=\"yt_notes_chunked\", min_chunk_size=150, max_chunk_size=500):\n",
    "        # Initialize models\n",
    "        self.genai_model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
    "        \n",
    "        # Adaptive chunking parameters\n",
    "        self.min_chunk_size = min_chunk_size\n",
    "        self.max_chunk_size = max_chunk_size\n",
    "        \n",
    "        # Initialize vector database\n",
    "        self.chroma_client = chromadb.PersistentClient(path=db_path)\n",
    "        self.gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=GEMINI_API_KEY)\n",
    "        self.collection = self.chroma_client.get_or_create_collection(\n",
    "            name=collection_name, \n",
    "            embedding_function=self.gemini_ef\n",
    "        )\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _adaptive_chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Dynamically adjust chunk size based on content density\"\"\"\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_length = len(sentence.split())\n",
    "            \n",
    "            if sentence_length > self.max_chunk_size:\n",
    "                chunks.append(sentence)  # Store long sentences as individual chunks\n",
    "                continue\n",
    "            \n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "            \n",
    "            if current_length >= self.min_chunk_size:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _extract_important_dialogues(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract key dialogues that are impactful or philosophical.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        important_dialogues = [sent for sent in sentences if len(sent.split()) > 8]\n",
    "        return important_dialogues\n",
    "\n",
    "    def _summarize_chunk(self, chunk: str) -> str:\n",
    "        \"\"\"Summarize a single chunk using Gemini AI\"\"\"\n",
    "        prompt = \"Summarize this section of the transcript while keeping important details: \"\n",
    "        response = self.genai_model.generate_content(prompt + chunk, stream=False)\n",
    "    \n",
    "        # Fix: The response structure might have changed\n",
    "        # Try accessing the text content correctly based on the response structure\n",
    "        if hasattr(response, 'text'):\n",
    "            return response.text\n",
    "        elif hasattr(response, 'parts'):\n",
    "            return response.parts[0].text\n",
    "        elif isinstance(response, dict) and 'candidates' in response:\n",
    "            # For newer API versions that return a dict\n",
    "            return response['candidates'][0]['content']['parts'][0]['text']\n",
    "        else:\n",
    "            # Fallback: convert the entire response to string\n",
    "            return str(response)       \n",
    "\n",
    "    def add_video(self, video_id: str, metadata: Dict[str, Any] = None) -> None:\n",
    "        \"\"\"Process a YouTube video, chunk its transcript dynamically, summarize chunks, extract key dialogues, and add to the database\"\"\"\n",
    "        try:\n",
    "            # Get transcript\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en','en-US','en-GB'])\n",
    "        \n",
    "            # Debug the transcript structure\n",
    "            if transcript:\n",
    "                print(f\"Transcript type: {type(transcript)}\")\n",
    "                if transcript:\n",
    "                    print(f\"First transcript item: {transcript[0]}\")\n",
    "                    print(f\"First transcript item type: {type(transcript[0])}\")\n",
    "        \n",
    "            # Convert transcript to text manually if it's a list of dictionaries\n",
    "            if isinstance(transcript, list) and transcript and isinstance(transcript[0], dict):\n",
    "                transcript_text = \"\\n\".join(item.get('text', '') for item in transcript)\n",
    "            else:\n",
    "                # Use the formatter as a fallback\n",
    "                transcript_text = TextFormatter().format_transcript(transcript)\n",
    "\n",
    "            # Perform adaptive chunking\n",
    "            chunks = self._adaptive_chunk_text(transcript_text)\n",
    "            print(f\"Split transcript into {len(chunks)} adaptive chunks\")\n",
    "\n",
    "            # Extract key dialogues\n",
    "            key_dialogues = self._extract_important_dialogues(transcript_text)\n",
    "\n",
    "            # Debug the response structure\n",
    "            if chunks:\n",
    "                test_response = self.genai_model.generate_content(\"Test prompt\", stream=False)\n",
    "                print(f\"Response type: {type(test_response)}\")\n",
    "                print(f\"Response dir: {dir(test_response)}\")\n",
    "                print(f\"Response repr: {repr(test_response)}\")\n",
    "\n",
    "            # Summarize each chunk\n",
    "            summaries = [self._summarize_chunk(chunk) for chunk in chunks]\n",
    "            combined_summary = \" \".join(summaries)\n",
    "\n",
    "            # Prepare for batch insertion\n",
    "            chunk_ids, chunk_texts, chunk_metadatas = [], [], []\n",
    "            base_metadata = metadata or {}\n",
    "            base_metadata[\"video_id\"] = video_id\n",
    "\n",
    "            for i, chunk in enumerate(summaries):  \n",
    "                chunk_id = f\"{video_id}_{i}\"\n",
    "                chunk_metadata = base_metadata.copy()\n",
    "                chunk_metadata[\"chunk_index\"] = i\n",
    "                chunk_metadata[\"chunk_count\"] = len(summaries)\n",
    "\n",
    "                chunk_ids.append(chunk_id)\n",
    "                chunk_texts.append(chunk)\n",
    "                chunk_metadatas.append(chunk_metadata)\n",
    "\n",
    "            # Insert into database\n",
    "            self.collection.upsert(\n",
    "                documents=chunk_texts + key_dialogues,\n",
    "                metadatas=chunk_metadatas + [{\"video_id\": video_id, \"type\": \"dialogue\"} for _ in key_dialogues],\n",
    "                ids=chunk_ids + [f\"{video_id}dialogue{i}\" for i in range(len(key_dialogues))]\n",
    "            )\n",
    "\n",
    "            print(f\"Successfully added video {video_id} with {len(summaries)} summarized adaptive chunks and {len(key_dialogues)} key dialogues\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_id}: {e}\")\n",
    "            # Print more detailed error information\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    \n",
    "    def answer_question(self, query: str, n_results: int = 5) -> str:\n",
    "        \"\"\"Perform hybrid search with ranking weights and retrieve key dialogues if available\"\"\"\n",
    "        queries = [query]  # Support for multi-part queries in future\n",
    "        \n",
    "        all_chunks = []\n",
    "        used_chunk_ids = set()\n",
    "        \n",
    "        for subquery in queries:\n",
    "            # Semantic Search\n",
    "            semantic_results = self.collection.query(\n",
    "                query_texts=[subquery],\n",
    "                n_results=n_results,\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            # Keyword Search (BM25-like)\n",
    "            keyword_results = self.collection.get(\n",
    "                where_document={\"$contains\": subquery},\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            # Retrieve key dialogues related to the topic\n",
    "            dialogue_results = self.collection.get(\n",
    "                where={\"type\": \"dialogue\"},\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "            \n",
    "            # Merge Results with Ranking Weights\n",
    "            ranked_results = [(doc, 0.7) for doc in semantic_results['documents'][0]]\n",
    "            ranked_results += [(doc, 0.3) for doc in keyword_results['documents']]\n",
    "            ranked_results += [(doc, 0.9) for doc in dialogue_results['documents']]  # Prioritize dialogues\n",
    "            \n",
    "            # Sort results by weight\n",
    "            ranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            for doc, _ in ranked_results:\n",
    "                chunk_id = f\"{query}_{len(all_chunks)}\"\n",
    "                if chunk_id not in used_chunk_ids:\n",
    "                    all_chunks.append(doc)\n",
    "                    used_chunk_ids.add(chunk_id)\n",
    "        \n",
    "        # Combine all retrieved chunks as context\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(all_chunks)\n",
    "        \n",
    "        # Generate response\n",
    "        prompt = f\"\"\"\n",
    "        QUESTION: {query}\n",
    "        \n",
    "        DOCUMENTS:\n",
    "        {combined_context}\n",
    "        \"\"\"\n",
    "        response = self.genai_model.generate_content(prompt, stream=False)\n",
    "        \n",
    "        # Handle the response consistently with the _summarize_chunk method\n",
    "        if hasattr(response, 'text'):\n",
    "            return response.text\n",
    "        elif hasattr(response, 'parts'):\n",
    "            return response.parts[0].text\n",
    "        elif isinstance(response, dict) and 'candidates' in response:\n",
    "            # For newer API versions that return a dict\n",
    "            return response['candidates'][0]['content']['parts'][0]['text']\n",
    "        else:\n",
    "            # Fallback: convert the entire response to string\n",
    "            return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing YouTube QA System...\n",
      "Processing 3 videos...\n",
      "\n",
      "Processing video: qsobCAGiuRU\n",
      "Transcript type: <class 'list'>\n",
      "First transcript item: {'text': '[speaking in Korean]', 'start': 0.03, 'duration': 3.083}\n",
      "First transcript item type: <class 'dict'>\n",
      "Split transcript into 7 adaptive chunks\n",
      "Response type: <class 'google.generativeai.types.generation_types.GenerateContentResponse'>\n",
      "Response dir: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_chunks', '_done', '_error', '_iterator', '_result', 'candidates', 'from_iterator', 'from_response', 'model_version', 'parts', 'prompt_feedback', 'resolve', 'text', 'to_dict', 'usage_metadata']\n",
      "Response repr: response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Okay, I'm ready. Please provide your test prompt! I will do my best to respond in a way that is:\\n\\n*   **Relevant:** I will address the core of your prompt.\\n*   **Accurate:** I will provide factual information if applicable.\\n*   **Helpful:** I will try to be useful and informative.\\n*   **Clear:** I will communicate my response in a way that is easy to understand.\\n*   **Concise:** I will be as brief as possible while still providing a complete answer.\\n\\nI'm looking forward to seeing what you have! Let's begin.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.31820241055747334\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 2,\n",
      "        \"candidates_token_count\": 129,\n",
      "        \"total_token_count\": 131\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.0-flash\"\n",
      "    }),\n",
      ")\n",
      "Successfully added video qsobCAGiuRU with 7 summarized adaptive chunks and 16 key dialogues\n",
      "\n",
      "Processing video: salY_Sm6mv4\n",
      "Error processing video salY_Sm6mv4: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=salY_Sm6mv4! This is most likely caused by:\n",
      "\n",
      "Subtitles are disabled for this video\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "\n",
      "Processing video: q4pDUxth5fQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\RUSHITA\\AppData\\Local\\Temp\\ipykernel_8460\\2278491354.py\", line 95, in add_video\n",
      "    transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en','en-US','en-GB'])\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RUSHITA\\Desktop\\originalweeb\\yt rag\\.venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py\", line 302, in get_transcript\n",
      "    cls.list_transcripts(video_id, proxies, cookies)\n",
      "  File \"c:\\Users\\RUSHITA\\Desktop\\originalweeb\\yt rag\\.venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py\", line 203, in list_transcripts\n",
      "    return ytt_api.list(video_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RUSHITA\\Desktop\\originalweeb\\yt rag\\.venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py\", line 131, in list\n",
      "    return self._fetcher.fetch(video_id)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RUSHITA\\Desktop\\originalweeb\\yt rag\\.venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py\", line 351, in fetch\n",
      "    self._fetch_captions_json(video_id),\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RUSHITA\\Desktop\\originalweeb\\yt rag\\.venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py\", line 356, in _fetch_captions_json\n",
      "    return self._extract_captions_json(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\RUSHITA\\Desktop\\originalweeb\\yt rag\\.venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py\", line 386, in _extract_captions_json\n",
      "    raise TranscriptsDisabled(video_id)\n",
      "youtube_transcript_api._errors.TranscriptsDisabled: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=salY_Sm6mv4! This is most likely caused by:\n",
      "\n",
      "Subtitles are disabled for this video\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript type: <class 'list'>\n",
      "First transcript item: {'text': 'Whether it’s being chained to a burning \\nwheel, turned into a spider,', 'start': 8.29, 'duration': 4.549}\n",
      "First transcript item type: <class 'dict'>\n",
      "Split transcript into 4 adaptive chunks\n",
      "Response type: <class 'google.generativeai.types.generation_types.GenerateContentResponse'>\n",
      "Response dir: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_chunks', '_done', '_error', '_iterator', '_result', 'candidates', 'from_iterator', 'from_response', 'model_version', 'parts', 'prompt_feedback', 'resolve', 'text', 'to_dict', 'usage_metadata']\n",
      "Response repr: response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Okay, I'm ready for your test prompt.  Please give me the instructions or question. I'll do my best to answer according to my capabilities.\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.36075103984159584\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 2,\n",
      "        \"candidates_token_count\": 34,\n",
      "        \"total_token_count\": 36\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.0-flash\"\n",
      "    }),\n",
      ")\n",
      "Successfully added video q4pDUxth5fQ with 4 summarized adaptive chunks and 26 key dialogues\n",
      "\n",
      "System initialization complete.\n",
      "You can now ask questions about the videos.\n"
     ]
    }
   ],
   "source": [
    "# Main execution script\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing YouTube QA System...\")\n",
    "    \n",
    "    # Initialize the system\n",
    "    qa_system = YouTubeQASystem()\n",
    "    \n",
    "    # Add videos (sample IDs)\n",
    "    sample_videos = [\n",
    "        \"qsobCAGiuRU\",  \n",
    "        \"salY_Sm6mv4\",  \n",
    "        \"q4pDUxth5fQ\"   \n",
    "    ]\n",
    "    \n",
    "    print(f\"Processing {len(sample_videos)} videos...\")\n",
    "    for video_id in sample_videos:\n",
    "        print(f\"\\nProcessing video: {video_id}\")\n",
    "        qa_system.add_video(video_id)\n",
    "    \n",
    "    print(\"\\nSystem initialization complete.\")\n",
    "    print(\"You can now ask questions about the videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what questions were asked to BTS? what is the myth of sisyphus?\n",
      "\n",
      "Answer:\n",
      "Here are the questions asked to BTS and a description of the myth of Sisyphus, based on the provided documents:\n",
      "\n",
      "**Questions Asked to BTS:**\n",
      "\n",
      "*   Can BTS eat spicy food?\n",
      "*   How can BTS sing and dance at the same time?\n",
      "*   How old are members in BTS?\n",
      "\n",
      "**The Myth of Sisyphus:**\n",
      "\n",
      "Sisyphus was the first king of Ephyra (Corinth), a clever but devious tyrant who angered the gods with his actions, including seducing his niece and killing visitors.\n",
      "\n",
      "He tricked the gods multiple times:\n",
      "\n",
      "1.  He revealed Zeus's kidnapping of Aegina to her father, the river god Asopus, in exchange for a spring in his city. This angered Zeus.\n",
      "2.  When Thanatos (Death) came to chain him in the underworld, Sisyphus tricked Thanatos and chained him instead, preventing anyone from dying.\n",
      "3.  After eventually dying, he convinced Persephone, queen of the Underworld, to let him return to the land of the living to punish his wife for not giving him a proper burial. However, he refused to return to the Underworld.\n",
      "\n",
      "As punishment for his deceit and defiance, Sisyphus was condemned to an eternal task: rolling a massive boulder up a hill, only to have it roll back down each time he neared the top, forcing him to start again indefinitely.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Answer a question\n",
    "question = \"what questions were asked to BTS? what is the myth of sisyphus?\"\n",
    "answer = qa_system.answer_question(question)\n",
    "    \n",
    "print(\"Question:\", question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
